{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from itertools import cycle\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "sns.set_theme(style=\"white\",palette=None)\n",
    "color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files_path = glob('./data/*/*/*.wav')     #用glob獲得檔案\n",
    "ipd.Audio(audio_files_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list=[]\n",
    "sr_list=[]\n",
    "for audio_file in audio_files_path:\n",
    "    y_list.append(librosa.load(audio_file)[0])   #音檔相對振幅\n",
    "    sr_list.append(librosa.load(audio_file)[1])  #sample_rate 採樣頻率\n",
    "#print('y.type: ',type(y),'\\t','sample rate: ',sr)\n",
    "print('number of datas: ',len(y_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將音檔無聲部分剪除，並把音檔長度固定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trimmed_list = []     #\n",
    "sr = sr_list[0]\n",
    "target_length = 5 * sr #目標音檔長度\n",
    "for y in y_list:\n",
    "        y_trimmed, _ = librosa.effects.trim(y,top_db=60)        #設定閾值80 \n",
    "        if len(y_trimmed)>target_length:\n",
    "                y_trimmed =  y_trimmed[:target_length]\n",
    "        y_trimmed_list.append(y_trimmed)\n",
    "for i  in range(2):\n",
    "    pd.Series( y_trimmed_list[i]).plot(figsize=(10,4),lw=1,title='Raw audio',color=color_pal[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "製作label=0的音檔(ECS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECS_files_path  = glob('./audio/*.wav')\n",
    "#ipd.Audio(ECS_files_path[1])       #聽音檔\n",
    "y0_list=[]\n",
    "sr0_list=[]\n",
    "for ECS_file in ECS_files_path:\n",
    "    y0_list.append(librosa.load(ECS_file)[0])   #音檔振幅\n",
    "    sr0_list.append(librosa.load(ECS_file)[1])  #sample_rate\n",
    "#for i in range(len(y0_list)):           \n",
    "#   if len(y0_list[i])!=110250:     #全部是5秒音檔\n",
    "#      print('no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "傅立葉轉換成可丟進CNN的圖檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#stft加上振幅轉分貝\n",
    "S_db_list = []\n",
    "for y_trimmed in y_trimmed_list:\n",
    "    D = librosa.stft(y_trimmed)\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D),ref=np.max)\n",
    "    S_db_list.append(S_db)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''for i in range(5):\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    img = librosa.display.specshow(S_db_list[i],\n",
    "                                   x_axis='time',\n",
    "                                   y_axis='log',\n",
    "                                   ax=ax\n",
    "                                   )\n",
    "    ax.set_title('Spectogram Example', fontsize=20)\n",
    "    fig.colorbar(img,ax=ax, format=f'%0.2f')\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用mel_spectogram轉成人類可聽音檔圖片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "S_list = []\n",
    "S_db_mel_list = []\n",
    "num_channels = 3 #指定圖片輸入通道數\n",
    "\n",
    "for i in range(len(y_trimmed_list)):        #轉換成梅爾頻譜圖\n",
    "        S = librosa.feature.melspectrogram(y=y_trimmed_list[i],\n",
    "                                           sr=sr_list[i],\n",
    "                                           n_mels=216)\n",
    "        S_list.append(S)\n",
    "        S_db_mel = librosa.amplitude_to_db(S,ref=np.max)    #把音檔從振幅轉成分貝\n",
    "        S_db_mel = np.asarray(torchvision.transforms.Resize((224, 224))(Image.fromarray(S_db_mel)))\n",
    "        S_db_mel_list.append(S_db_mel)\n",
    "        \n",
    "for i  in range(1):             #印出梅爾頻譜圖\n",
    "    fig, ax = plt.subplots(figsize=(6,3))\n",
    "    img = librosa.display.specshow(S_db_mel_list[i],\n",
    "                                   x_axis='time',\n",
    "                                   y_axis='log',\n",
    "                                   ax=ax\n",
    "                                  )\n",
    "    ax.set_title('MelSpectogram Example', fontsize=20)\n",
    "    fig.colorbar(img,ax=ax, format=f'%0.2f')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_size = [128,256,512]\n",
    "win_size = [100,200,400]\n",
    "S_db_channels_list = []\n",
    "for i in range(len(y_trimmed_list)):        #轉換成梅爾頻譜圖\n",
    "    S_db_channels = []\n",
    "    for j in range(num_channels):\n",
    "        S = librosa.feature.melspectrogram(y=y_trimmed_list[i],\n",
    "                                           sr=sr_list[i],\n",
    "                                           hop_length = hop_size[j],\n",
    "                                           win_length=win_size[j]\n",
    "                                           )\n",
    "        S_db_mel = librosa.amplitude_to_db(S,ref=np.max)    #把音檔從振幅轉成分貝\n",
    "        S_db_mel = np.asarray(torchvision.transforms.Resize((224, 224))(Image.fromarray(S_db_mel)))\n",
    "        S_db_channels.append(S_db_mel)                          \n",
    "    S_db_channels_arr = np.array(S_db_channels)\n",
    "    S_db_channels_list.append( S_db_channels_arr)\n",
    "print(S_db_channels_list[0].shape)      #一個list，每個元素是3*224*224的梅爾頻譜圖 數據集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將頻譜圖大小限制在216*216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  i in range(len(y_list)):\n",
    "    #print(S_db_mel_list[i].shape)\n",
    "    if S_db_mel_list[i].shape[1]<216:\n",
    "        print(audio_files_path[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "製作ECS(label=0)的mel_spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_size = [128,256,512]\n",
    "win_size = [100,200,400]\n",
    "S0_db_channels_list = []\n",
    "for i in range(len(y0_list)):        #轉換成梅爾頻譜圖\n",
    "    S0_db_channels = []\n",
    "    for j in range(num_channels):\n",
    "        S0 = librosa.feature.melspectrogram(y=y0_list[i],\n",
    "                                           sr=sr0_list[i],\n",
    "                                           hop_length = hop_size[j],\n",
    "                                           win_length=win_size[j]\n",
    "                                           )\n",
    "        S0_db_mel = librosa.amplitude_to_db(S0,ref=np.max)    #把音檔從振幅轉成分貝\n",
    "        S0_db_mel = np.asarray(torchvision.transforms.Resize((224, 224))(Image.fromarray(S0_db_mel)))\n",
    "        S0_db_channels.append(S0_db_mel)                          \n",
    "    S0_db_channels_arr = np.array(S0_db_channels)\n",
    "    S0_db_channels_list.append( S0_db_channels_arr)\n",
    "print(S0_db_channels_list[0].shape)      #一個list，每個元素是3*224*224的梅爾頻譜圖 數據集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合併成數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for  i in range(len(y_list)):                       #label=1放入 456筆\n",
    "    new_entry = {}\n",
    "    new_entry['values'] = S_db_channels_list[i]\n",
    "    new_entry['target'] = 1\n",
    "    new_entry['audio'] = y_list[i]\n",
    "    dataset.append(new_entry)\n",
    "\n",
    "for  i in range(len(y0_list)):                      #label=0放入 411筆\n",
    "    new_entry = {}\n",
    "    new_entry['values'] = S0_db_channels_list[i]\n",
    "    new_entry['target'] = 0\n",
    "    new_entry['audio'] = y0_list[i]\n",
    "    dataset.append(new_entry)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset[0]['values'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成1個channel梅爾頻譜圖的數據集(label=1，values是時頻圖(大小為216*216)，audio是音檔振幅array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split \n",
    "import torch\n",
    "\n",
    "#切割訓練集、測試集\n",
    "featureset =[entry['values'] for entry in dataset]\n",
    "targetset = [entry['target'] for entry in dataset]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(featureset,targetset,test_size=0.2,random_state=42)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, features, targets):  #輸入切分數據集的特徵和標籤\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)   #返回資料集長度\n",
    "\n",
    "    def __getitem__(self, idx):     #返回梅爾頻譜圖和標籤\n",
    "        feature = self.features[idx]\n",
    "        target = self.targets[idx]\n",
    "        return feature, target\n",
    "\n",
    "trainset = AudioDataset(X_train,Y_train) \n",
    "testset = AudioDataset(X_test,Y_test) \n",
    "train_Loader = DataLoader(trainset,batch_size=32,shuffle=True,num_workers=2)\n",
    "test_Loader = DataLoader(testset,batch_size=32,shuffle=False,num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.__getitem__(15)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立模型(使用預訓練的densenet作微調)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        num_classes = 2     #二元分類\n",
    "        self.model = models.densenet121(pretrained=True) #選用densenet121\n",
    "        num_features = self.model.classifier.in_features       #取得classfier層的輸入\n",
    "        self.model.classifier = nn.Linear(num_features, num_classes)    #修改成自己的分類\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luffy/anaconda3/envs/final_project_m1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/luffy/anaconda3/envs/final_project_m1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "net = DenseNet() #desnet作二元分類\n",
    "criterion = nn.CrossEntropyLoss()               #loss function\n",
    "optimizer= optim.SGD(net.parameters(),lr=0.001,momentum=0.9)  #設定優化方法(提供模型參數、學習率、momentum幫助沖出local minimum)\n",
    "\n",
    "def train_model(model,dataloader,lossfn,optimizer):  #模型訓練\n",
    "    epochs = 10\n",
    "    model.train()\n",
    "    for epoch in range(epochs):        #跑epoch\n",
    "        running_loss = 0.0      #紀錄loss\n",
    "        with tqdm(total=len(dataloader)) as t:  #做進度條\n",
    "                \n",
    "            for i, data in enumerate(train_Loader, 0):      #enumerate用於將一個可遍歷的數列作編號，0表示從0開始編\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "                # zero the parameter gradients 讓梯度先歸零，才能作下次計算\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                loss = lossfn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                t.update()\n",
    "                # print statistics\n",
    "                running_loss += loss.item() ##加上損失函數\n",
    "                if i % 200 == 199:    # print every 200 mini-batches\n",
    "                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "                    running_loss = 0.0\n",
    "    model.eval()\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:31<00:00,  1.42s/it]\n",
      "100%|██████████| 22/22 [00:32<00:00,  1.45s/it]\n",
      "100%|██████████| 22/22 [00:30<00:00,  1.38s/it]\n",
      "100%|██████████| 22/22 [00:30<00:00,  1.39s/it]\n",
      "100%|██████████| 22/22 [00:32<00:00,  1.47s/it]\n",
      "100%|██████████| 22/22 [00:30<00:00,  1.38s/it]\n",
      "100%|██████████| 22/22 [00:30<00:00,  1.39s/it]\n",
      "100%|██████████| 22/22 [00:30<00:00,  1.38s/it]\n",
      "100%|██████████| 22/22 [00:30<00:00,  1.38s/it]\n",
      "100%|██████████| 22/22 [00:30<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(net,train_Loader,criterion,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './densenet.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_Loader)\n",
    "images, labels = next(dataiter)\n",
    "net = DenseNet()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9375\n"
     ]
    }
   ],
   "source": [
    "k=[]\n",
    "acc = 0.\n",
    "total = 0.\n",
    "for output in outputs:\n",
    "    if output[0]>output[1]:\n",
    "        k.append(0)\n",
    "    else:\n",
    "        k.append(1)\n",
    "for i in range(len(labels)):\n",
    "    if k[i] == labels[i].item():\n",
    "        acc+=1\n",
    "    total+=1\n",
    "print('accuracy: ',acc/total)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
